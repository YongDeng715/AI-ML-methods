{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def get_activation_func(name: str):\n",
    "    if name == 'sigmoid':\n",
    "        return lambda x: 1 / (1 + np.exp(-x))\n",
    "    elif name == 'relu':\n",
    "        return lambda x: np.maximum(0, x)\n",
    "    elif name == 'tanh':\n",
    "        return lambda x: np.tanh(x)\n",
    "    else:\n",
    "        raise KeyError(f'No such activation function: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: A 1-D numpy array.\n",
    "    Returns:\n",
    "        A 1-D numpy array containing the softmax values.\n",
    "    \"\"\"\n",
    "    exp_vals = np.exp(x - np.max(x)) \n",
    "    return exp_vals / np.sum(exp_vals, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "ReLU: [0 0 0 1 2]\n",
      "Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "Softmax: [0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the sigmoid activation function\n",
    "sigmoid_func = get_activation_func('sigmoid')\n",
    "\n",
    "# Get the ReLU activation function\n",
    "relu_func = get_activation_func('relu')\n",
    "\n",
    "# Get the tanh activation function\n",
    "tanh_func = get_activation_func('tanh')\n",
    "\n",
    "# Test the activation functions\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"Sigmoid:\", sigmoid_func(x))\n",
    "print(\"ReLU:\", relu_func(x))\n",
    "print(\"Tanh:\", tanh_func(x))\n",
    "print(\"Softmax:\", softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.input_dim = input_dim\n",
    "        self.gamma = np.ones((1, input_dim))  # Scale parameter\n",
    "        self.beta = np.zeros((1, input_dim))  # Shift parameter\n",
    "        self.running_mean = np.zeros((1, input_dim))\n",
    "        self.running_var = np.ones((1, input_dim))\n",
    "    \n",
    "    def forward(self, x, is_training=True):\n",
    "        if is_training:\n",
    "            # Calculate mean and variance along the batch axis\n",
    "            mean = np.mean(x, axis=0, keepdims=True)\n",
    "            var = np.var(x, axis=0, keepdims=True)\n",
    "            \n",
    "            # Normalize the input\n",
    "            x_normalized = (x - mean) / np.sqrt(var + self.epsilon)\n",
    "            \n",
    "            # Update running mean and variance using momentum\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            # Use running mean and variance during inference\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_normalized + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, D = dout.shape\n",
    "        \n",
    "        # Calculate gradients for gamma and beta\n",
    "        dgamma = np.sum(dout * x_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dout, axis=0, keepdims=True)\n",
    "        \n",
    "        # Calculate gradient of normalized input\n",
    "        dx_normalized = dout * self.gamma\n",
    "        \n",
    "        # Calculate gradients for mean and variance\n",
    "        dvar = np.sum(dx_normalized * (x - mean) * -0.5 * (var + self.epsilon)**(-1.5), axis=0, keepdims=True)\n",
    "        dmean = np.sum(dx_normalized * -1 / np.sqrt(var + self.epsilon), axis=0, keepdims=True) + \\\n",
    "                dvar * np.mean(-2 * (x - mean), axis=0, keepdims=True)\n",
    "        \n",
    "        # Calculate gradient of input\n",
    "        dx = dx_normalized / np.sqrt(var + self.epsilon) + dvar * 2 * (x - mean) / N + dmean / N\n",
    "        \n",
    "        return dx, dgamma, dbeta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
