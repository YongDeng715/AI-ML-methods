{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "visualize the data in terms of bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def download_dataset(url, filename):\n",
    "    data = requests.get(url)\n",
    "    with open(filename, 'wb') as code:\n",
    "        code.write(data.content)\n",
    "    print(f\"Downloading the datasets {filename} complete.\")\n",
    "    data_file = zipfile.ZipFile(filename, 'r')\n",
    "    data_list = data_file.namelist()\n",
    "\n",
    "    for file in data_list:\n",
    "        data_file.extract(file, 'd:/Desktop/AI-ML-methods/homework/assignment2/')\n",
    "    data_file.close()\n",
    "    print(f\"Unzipping the datasets {filename} complete.\")\n",
    "\n",
    "url = 'http://www.nustm.cn/member/rxia/ml/data/Tsinghua.zip'\n",
    "download_dataset(url, 'Tsinghua.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NultinomialNB\n",
    "\n",
    "Multinomial Distribution Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class MultinomialNBC(object):\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Count occurrences of each term in each class\n",
    "        count_vect = CountVectorizer()\n",
    "        X_counts = count_vect.fit_transform(X)\n",
    "        \n",
    "        self.feature_names = count_vect.get_feature_names_out()\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.class_counts = np.zeros(n_classes)\n",
    "        self.feature_counts = np.zeros((n_classes, len(self.feature_names)))\n",
    "\n",
    "        for i, cls_ in enumerate(self.classes):\n",
    "            cls_indices = np.where(y == cls_)[0]\n",
    "            self.class_counts[i] = len(cls_indices)\n",
    "            self.feature_counts[i] = np.array(X_counts[cls_indices].sum(axis=0)).flatten()\n",
    "\n",
    "        # Calculate class priors and feature probabilities\n",
    "        self.class_prior_ = self.class_counts / np.sum(self.class_counts)\n",
    "        self.feature_log_prob_ = np.log((self.feature_counts + self.alpha) /\n",
    "                                        (np.sum(self.feature_counts, axis=1, keepdims=True) +\n",
    "                                         self.alpha * len(self.feature_names)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Transform input documents into counts\n",
    "        count_vect = CountVectorizer(vocabulary=self.feature_names)\n",
    "        X_counts = count_vect.fit_transform(X)\n",
    "\n",
    "        # Calculate log likelihood of each class for each document\n",
    "        log_likelihood = np.dot(X_counts, self.feature_log_prob_.T) + np.log(self.class_prior_)\n",
    "\n",
    "        # Predict the class with the highest log likelihood\n",
    "        return self.classes[np.argmax(log_likelihood, axis=1)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BernoulliNB\n",
    "\n",
    "Multi-variate Bernoulli Distribution Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNBC(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Transform input documents into binary vectors\n",
    "        count_vect = CountVectorizer(binary=True)\n",
    "        X_bin = count_vect.fit_transform(X)\n",
    "        self.feature_names = count_vect.get_feature_names_out()\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.class_counts = np.zeros(n_classes)\n",
    "        self.feature_counts = np.zeros((n_classes, len(self.feature_names)))\n",
    "\n",
    "        for i, cls_ in enumerate(self.classes):\n",
    "            cls_indices = np.where(y == cls_)[0]\n",
    "            self.class_counts[i] = len(cls_indices)\n",
    "            self.feature_counts[i] = np.array(X_bin[cls_indices].sum(axis=0)).flatten()\n",
    "\n",
    "        # Calculate class priors and feature probabilities\n",
    "        self.class_prior_ = self.class_counts / np.sum(self.class_counts)\n",
    "        self.feature_log_prob_ = np.log((self.feature_counts + self.alpha) /\n",
    "                                        (self.class_counts.reshape(-1, 1) + self.alpha * 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Transform input documents into binary vectors\n",
    "        count_vect = CountVectorizer(binary=True, vocabulary=self.feature_names)\n",
    "        X_bin = count_vect.fit_transform(X)\n",
    "\n",
    "        # Calculate log likelihood of each class for each document\n",
    "        log_likelihood = np.dot(X_bin, self.feature_log_prob_.T) + np.log(self.class_prior_)\n",
    "\n",
    "        # Predict the class with the highest log likelihood\n",
    "        return self.classes[np.argmax(log_likelihood, axis=1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocession\n",
    "\n",
    "\n",
    "+ 加载数据集： 从原始数据集中加载文本数据。\n",
    "+ 分词处理： 使用分词工具（如 jieba）对文本数据进行分词处理，并过滤停用词。\n",
    "+ 划分训练集和测试集： 将分词处理后的数据集划分为训练集和测试集。\n",
    "+ 保存训练集和测试集： 将划分后的训练集和测试集保存到指定的目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# 停词处理\n",
    "def segment_text(text, stopwords):\n",
    "    for j in range(text.shape[0]):\n",
    "        for word in stopwords:\n",
    "            if word in text[j]:\n",
    "                text[j] = text[j].replace(word, '')\n",
    "    # print(f\"info of text: {type(text)}, {text.shape}, \n",
    "    #       info of stopwords: {type(stopwords)}, {len(stopwords)}\")\n",
    "    return text\n",
    "\n",
    "# 加载停用词列表\n",
    "def load_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "    return set(stopwords)\n",
    "\n",
    "# 加载数据集\n",
    "label_mapping = defaultdict(int) # label mapping dictionary\n",
    "\n",
    "def load_data(data_dir, stopwords_file):\n",
    "    X = []\n",
    "    X_total = []\n",
    "    y = []\n",
    "\n",
    "    # Mapping to convert labels to integers\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "    categories = os.listdir(data_dir)\n",
    "    global label_mapping\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        # 获取文件名作为label, label不在字典 label_mapping 中时，将其添加字典中\n",
    "        label = os.path.splitext(category)[0]     \n",
    "        if label not in label_mapping:\n",
    "            label_mapping[label] = len(label_mapping)\n",
    "        \n",
    "        file_path = os.path.join(data_dir, category)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # 读取文件中的每一行，并将其存储为 NumPy 数组\n",
    "            text = np.array(file.read().splitlines())\n",
    "        # 移除停用词\n",
    "        text = segment_text(text, stopwords)\n",
    "\n",
    "        _b1 = text != '<text>' \n",
    "        _b2 = text != '</text>'\n",
    "        # text 数组的对应元素同时满足 _b1 和 _b2 的条件\n",
    "        _b = np.all(np.c_[_b1, _b2], axis=1)\n",
    "        # print(f\"info of skiping </text>: {type(_b)}, {_b.shape}\")\n",
    "        text = text[_b]\n",
    "        # print(text) # 经过处理后的文本 numpy数组\n",
    "\n",
    "        for sentence in text:\n",
    "            # 一个句子 sentence 按照空格进行分割，\n",
    "            words = sentence.split()\n",
    "            X_total += words\n",
    "            X.append(words)\n",
    "            # y.append(label_mapping[label])\n",
    "        y += [label_mapping[label]] * len(text)\n",
    "        # print(len(X), len(y), len(X_total))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_train_data = '../Tsinghua/train/'\n",
    "path_to_test_data = '../Tsinghua/test/'\n",
    "stopwords_file = '../Tsinghua/stop_words_zh.txt'\n",
    "\n",
    "X_train, y_train = load_data(path_to_train_data, stopwords_file)\n",
    "X_test, y_test = load_data(path_to_test_data, stopwords_file)\n",
    "\n",
    "print(type(X_train), len(X_train))\n",
    "print(type(y_train), len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vector(X, label_mapping, method):\n",
    "    assert method in ['bernulli', 'multinomial'], 'Method must be either \"bernulli\" or \"multinomial\"'\n",
    "    \n",
    "    # Shape of word_matrix: (number of sentences, number of labels)\n",
    "    word_matrix = np.zeros((len(X), len(label_mapping)))\n",
    "    for i, sentence in enumerate(X):\n",
    "        for word in sentence:\n",
    "            if word in label_mapping.keys():\n",
    "                if method == 'bernulli':\n",
    "                    word_matrix[i, label_mapping[word]] = 1 \n",
    "                elif method == 'multinomial':\n",
    "                    word_matrix[i, label_mapping[word]] += 1\n",
    "    return word_matrix\n",
    "\n",
    "word_matrix_tb = create_word_vector(X_train, label_mapping, 'bernulli')\n",
    "word_matrix_tm = create_word_vector(X_train, label_mapping, 'multinomial')\n",
    "word_matrix_pb = create_word_vector(X_test, label_mapping, 'bernulli')\n",
    "word_matrix_pm = create_word_vector(X_test, label_mapping, 'multinomial')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
